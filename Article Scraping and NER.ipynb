{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article #newspaper has some nlp stuff, don't know if it is good enough\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/scrape-and-summarize-news-articles-in-5-lines-of-python-code-175f0e5c7dfc\n",
    "url = \"https://thenewstack.io/dont-mess-with-the-master-working-with-branches-in-git-and-github/\"\n",
    "article = Article(url, language='en', fetch_images=False)\n",
    "\n",
    "#download and parse the article\n",
    "article.download()\n",
    "article.parse()\n",
    "#article.nlp() #Don't know if we need to parse\n",
    "\n",
    "text = article.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'ORG': 14,\n",
       "         'GPE': 2,\n",
       "         'CARDINAL': 11,\n",
       "         'NORP': 4,\n",
       "         'ORDINAL': 2,\n",
       "         'WORK_OF_ART': 3,\n",
       "         'PERSON': 1})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "#displacy.render(nlp(str(sentences[20])), jupyter=True, style='ent')\n",
    "\n",
    "labels = [x.label_ for x in doc.ents]\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instead, everyone uses branches created from master to experiment, make edits and additions and changes, before eventually rolling that branch back into the master once they have been approved and are known to work.\n"
     ]
    }
   ],
   "source": [
    "sentences = [x for x in doc.sents]\n",
    "print(sentences[21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello_octo', 10),\n",
       " ('GitHub', 9),\n",
       " ('one', 3),\n",
       " ('Github.com', 1),\n",
       " ('more than one', 1),\n",
       " ('One', 1),\n",
       " ('1', 1),\n",
       " ('first', 1),\n",
       " ('second', 1),\n",
       " ('2', 1),\n",
       " ('Hello World', 1),\n",
       " ('two', 1),\n",
       " ('git checkout hello_octo', 1),\n",
       " ('git checkout hello_kitty', 1),\n",
       " ('Git', 1),\n",
       " ('3', 1),\n",
       " ('4', 1),\n",
       " ('5', 1)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = [x.text for x in doc.ents]\n",
    "Counter(items).most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python3 -m nltk.downloader wordnet\n",
    "#! python3 -m nltk.downloader omw\n",
    "#! pip3 install spacy-wordnet\n",
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello_octo\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "\n",
      "GitHub\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "\n",
      "one\n",
      "[Synset('one.n.01'), Synset('one.n.02'), Synset('one.s.01'), Synset('one.s.02'), Synset('one.s.03'), Synset('one.s.04'), Synset('one.s.05'), Synset('one.s.06'), Synset('matchless.s.01')]\n",
      "[Lemma('one.n.01.one'), Lemma('one.n.01.1'), Lemma('one.n.01.I'), Lemma('one.n.01.ace'), Lemma('one.n.01.single'), Lemma('one.n.01.unity'), Lemma('one.n.02.one'), Lemma('one.s.01.one'), Lemma('one.s.01.1'), Lemma('one.s.01.i'), Lemma('one.s.01.ane'), Lemma('one.s.02.one'), Lemma('one.s.02.unitary'), Lemma('one.s.03.one'), Lemma('one.s.04.one'), Lemma('one.s.05.one'), Lemma('one.s.06.one'), Lemma('matchless.s.01.matchless'), Lemma('matchless.s.01.nonpareil'), Lemma('matchless.s.01.one'), Lemma('matchless.s.01.one_and_only'), Lemma('matchless.s.01.peerless'), Lemma('matchless.s.01.unmatched'), Lemma('matchless.s.01.unmatchable'), Lemma('matchless.s.01.unrivaled'), Lemma('matchless.s.01.unrivalled')]\n",
      "['number', 'number', 'philosophy', 'philosophy', 'philosophy']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#nlp.add_pipe(WordnetAnnotator(nlp.lang), after='tagger')\n",
    "for (item, count) in Counter(items).most_common():\n",
    "    token = nlp(item)[0]\n",
    "    print(token)\n",
    "    print(token._.wordnet.synsets())\n",
    "    print(token._.wordnet.lemmas())\n",
    "    print(token._.wordnet.wordnet_domains())\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/22031968/how-to-find-distance-between-two-synset-using-python-nltk-in-wordnet-hierarchy\n",
    "# want to do this next"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
