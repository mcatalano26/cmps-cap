{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article #newspaper has some nlp stuff, don't know if it is good enough\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/scrape-and-summarize-news-articles-in-5-lines-of-python-code-175f0e5c7dfc\n",
    "url = \"https://thenewstack.io/dont-mess-with-the-master-working-with-branches-in-git-and-github/\"\n",
    "article = Article(url, language='en', fetch_images=False)\n",
    "\n",
    "#download and parse the article\n",
    "article.download()\n",
    "article.parse()\n",
    "#article.nlp() #Don't know if we need to parse\n",
    "\n",
    "text = article.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'ORG': 14,\n",
       "         'GPE': 2,\n",
       "         'CARDINAL': 11,\n",
       "         'NORP': 4,\n",
       "         'ORDINAL': 2,\n",
       "         'WORK_OF_ART': 3,\n",
       "         'PERSON': 1})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "#displacy.render(nlp(str(sentences[20])), jupyter=True, style='ent')\n",
    "\n",
    "labels = [x.label_ for x in doc.ents]\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instead, everyone uses branches created from master to experiment, make edits and additions and changes, before eventually rolling that branch back into the master once they have been approved and are known to work.\n"
     ]
    }
   ],
   "source": [
    "sentences = [x for x in doc.sents]\n",
    "print(sentences[21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello_octo', 10), ('GitHub', 9), ('one', 3)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = [x.text for x in doc.ents]\n",
    "Counter(items).most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Sam/anaconda3/lib/python3.8/runpy.py:127: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package wordnet to /Users/Sam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/Users/Sam/anaconda3/lib/python3.8/runpy.py:127: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package omw to /Users/Sam/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n",
      "Requirement already satisfied: spacy-wordnet in /Users/Sam/anaconda3/lib/python3.8/site-packages (0.0.4)\n",
      "Requirement already satisfied: nltk<3.4,>=3.3 in /Users/Sam/anaconda3/lib/python3.8/site-packages (from spacy-wordnet) (3.3)\n",
      "Requirement already satisfied: six in /Users/Sam/anaconda3/lib/python3.8/site-packages (from nltk<3.4,>=3.3->spacy-wordnet) (1.15.0)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E007] 'WordnetAnnotator' already exists in pipeline. Existing names: ['tagger', 'WordnetAnnotator', 'parser', 'ner']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-166f5411df10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' python3 -m nltk.downloader omw'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' pip3 install spacy-wordnet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWordnetAnnotator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tagger'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy_wordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordnet_annotator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordnetAnnotator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[0;34m(self, component, name, before, after, first, last)\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_component_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE007\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbefore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mafter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE006\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E007] 'WordnetAnnotator' already exists in pipeline. Existing names: ['tagger', 'WordnetAnnotator', 'parser', 'ner']"
     ]
    }
   ],
   "source": [
    "! python3 -m nltk.downloader wordnet\n",
    "! python3 -m nltk.downloader omw\n",
    "! pip3 install spacy-wordnet\n",
    "nlp.add_pipe(WordnetAnnotator(nlp.lang), after='tagger')\n",
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello_octo\n",
      "[]\n",
      "\n",
      "GitHub\n",
      "[]\n",
      "\n",
      "one\n",
      "[Synset('one.n.01'), Synset('one.n.02'), Synset('one.s.01'), Synset('one.s.02'), Synset('one.s.03'), Synset('one.s.04'), Synset('one.s.05'), Synset('one.s.06'), Synset('matchless.s.01')]\n",
      "\n",
      "Github.com\n",
      "[]\n",
      "\n",
      "more\n",
      "[Synset('more.a.01'), Synset('more.a.02')]\n",
      "\n",
      "One\n",
      "[Synset('one.n.01'), Synset('one.n.02'), Synset('one.s.01'), Synset('one.s.02'), Synset('one.s.03'), Synset('one.s.04'), Synset('one.s.05'), Synset('one.s.06'), Synset('matchless.s.01')]\n",
      "\n",
      "1\n",
      "[Synset('one.n.01'), Synset('one.s.01')]\n",
      "\n",
      "first\n",
      "[Synset('first.r.01'), Synset('first.r.02'), Synset('first.r.03'), Synset('foremost.r.01')]\n",
      "\n",
      "second\n",
      "[Synset('second.s.01'), Synset('second.a.02')]\n",
      "\n",
      "2\n",
      "[Synset('two.n.01'), Synset('two.s.01')]\n",
      "\n",
      "Hello\n",
      "[Synset('hello.n.01')]\n",
      "\n",
      "two\n",
      "[Synset('two.n.01'), Synset('deuce.n.04'), Synset('two.s.01')]\n",
      "\n",
      "git\n",
      "[Synset('rotter.n.01')]\n",
      "\n",
      "git\n",
      "[Synset('rotter.n.01')]\n",
      "\n",
      "Git\n",
      "[Synset('rotter.n.01')]\n",
      "\n",
      "3\n",
      "[Synset('three.n.01'), Synset('three.s.01')]\n",
      "\n",
      "4\n",
      "[Synset('four.n.01'), Synset('four.s.01')]\n",
      "\n",
      "5\n",
      "[Synset('five.n.01'), Synset('five.s.01')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for (item, count) in Counter(items).most_common():\n",
    "    token = nlp(item)[0]\n",
    "    print(token)\n",
    "    print(token._.wordnet.synsets())\n",
    "    #print(token._.wordnet.lemmas())\n",
    "    #print(token._.wordnet.wordnet_domains())\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/22031968/how-to-find-distance-between-two-synset-using-python-nltk-in-wordnet-hierarchy\n",
    "# want to do this next"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
