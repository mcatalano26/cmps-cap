{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, this notebook can be implemented seamlessly into the web app's back end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries and connect to reddit first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mattc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "mattcat26\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "CLIENT_ID = os.getenv('CLIENT_ID')\n",
    "CLIENT_SECRET=os.getenv('CLIENT_SECRET')\n",
    "APP_NAME=os.getenv('APP_NAME')\n",
    "REDDIT_USERNAME=os.getenv('REDDIT_USERNAME')\n",
    "REDDIT_PASSWORD=os.getenv('REDDIT_PASSWORD')\n",
    "\n",
    "import praw\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator\n",
    "nlp.add_pipe(WordnetAnnotator(nlp.lang), after='tagger')\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "STOP_WORDS = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "from newspaper import Article\n",
    "from newspaper import Config\n",
    "\n",
    "user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'\n",
    "config = Config()\n",
    "config.browser_user_agent = user_agent\n",
    "\n",
    "reddit = praw.Reddit(client_id=CLIENT_ID, client_secret=CLIENT_SECRET, user_agent=APP_NAME, username=REDDIT_USERNAME, password=REDDIT_PASSWORD)\n",
    "\n",
    "print(reddit.user.me())\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = pd.read_csv('files/compiled_comments_2_25_2021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_train_test_split(df, feature_list, target_name, test_size):\n",
    "    X = df[feature_list]\n",
    "    X = X.to_numpy()\n",
    "    y = df[target_name]\n",
    "    y = y.to_numpy()\n",
    "    rand_state = random.randint(0, 1000)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state=rand_state)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random Forest Classifier\n",
    "def random_forest_class_func(df, feature_list, target_name, test_size, estimators):\n",
    "    #set up training and testing split\n",
    "    X_train, X_test, y_train, y_test = set_up_train_test_split(df, feature_list, target_name, test_size)\n",
    "    \n",
    "    #fit ridge classifier to x and y training set\n",
    "    clf = RandomForestClassifier(n_estimators = estimators).fit(X_train, y_train)\n",
    "    \n",
    "    return clf\n",
    "\n",
    "features = ['tfidf', 'WordScore', 'WholeScore', 'contains_url', 'adjWordScore', 'no_url_WordScore', 'no_url_WholeScore', 'WordScoreNoStop', 'WholeScoreNoStop', 'no_url_or_stops_WholeScore', 'no_url_or_stops_WordScore']\n",
    "our_model = random_forest_class_func(comments_df, features, 'action', 0.1, 1000)\n",
    "our_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to take in the comment and article we want to predict on and acquire all features for the comment. This will be the order in which we collect features:\n",
    " - contains url feature\n",
    " - tfidf\n",
    " - WordScore\n",
    " - WholeScore\n",
    " - adjWordScore\n",
    " - no_url_WordScore\n",
    " - no_url_WholeScore\n",
    " - WordScoreNoStop\n",
    " - WholeScoreNoStop\n",
    " - no_url_or_stops_WordScore\n",
    " - no_url_or_stops_WholeScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To demonstrate the backend, we will hardcode a comment and article\n",
    "comment = 'this is a comment'\n",
    "comment = 'In a statement to NBC News, the Office of tje Director of National Intelligence said it will not interface with the Biden transition until the General Services Administration decides its clear who won'\n",
    "reddit_url = 'https://www.reddit.com/r/neutralnews/comments/jrts8z/biden_not_getting_intel_reports_because_trump/'\n",
    "\n",
    "def clean_article(article_url):\n",
    "    try:\n",
    "        article = Article(article_url, language='en', fetch_images=False, config = config)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        art_text = article.text\n",
    "        art_doc = nlp(art_text.lower())\n",
    "    except:\n",
    "        print('The article could not be cleaned')\n",
    "    return art_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_url_feature(comment):\n",
    "    urlarr = []\n",
    "    urlarr = re.findall('[(]?http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', str(com['content']))\n",
    "    if not urlarr:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_feature(comment, article):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comment_text is the the comment and art_doc is the cleaned text of the article\n",
    "def wordscore_feature(comment_text, art_doc):\n",
    "    art_doc = nlp(str(art_doc))\n",
    "    art_items = [x.text for x in art_doc.ents]\n",
    "    #get tokens\n",
    "    art_tokens = []\n",
    "    for (item, count) in Counter(art_items).most_common(5):\n",
    "        token = nlp(item)[0]\n",
    "        art_tokens += [token]\n",
    "        \n",
    "    doc = nlp(str(comment_text).lower())\n",
    "    items = [x.text for x in doc.ents]\n",
    "    \n",
    "    score = 0\n",
    "    \n",
    "    for (item, count) in Counter(items).most_common(5):\n",
    "        \n",
    "        token = nlp(item)\n",
    "        \n",
    "        wordScores = []\n",
    "        \n",
    "        for art_word in art_tokens:\n",
    "            \n",
    "            wordScores += [art_word.similarity(token)]\n",
    "            \n",
    "            if len(wordScores) != 0:\n",
    "                score += sum(wordScores)/len(wordScores)\n",
    "            else:\n",
    "                score = 0\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comment_text is the the comment and art_doc is the cleaned text of the article\n",
    "def wholescore_feature(comment_text, art_doc):\n",
    "    art_doc = nlp(str(art_doc))\n",
    "    comment_text = str(comment_text).lower()\n",
    "    doc = nlp(comment_text)\n",
    "    score = art_doc.similarity(doc)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjwordscore_feature(comment_text, art_doc):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    urlarr = re.findall('[(]?http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n",
    "    for url in urlarr:\n",
    "        text = text.replace(url, '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def big_func(comment_text, reddit_url):\n",
    "    feature_values = {'tfidf': 0, 'WordScore': 0, 'WholeScore': 0, 'contains_url': False, 'adjWordScore': 0, 'no_url_WordScore': 0, 'no_url_WholeScore': 0, 'WordScoreNoStop': 0, 'WholeScoreNoStop': 0, 'no_url_or_stops_WholeScore': 0, 'no_url_or_stops_WordScore': 0}\n",
    "    submission = reddit.submission(url = reddit_url)\n",
    "    article_url = submission.url\n",
    "    cleaned_article_text = clean_article(article_url)\n",
    "    \n",
    "    feature_values['contains_url'] = contains_url_feature(comment_text)\n",
    "    feature_values['tfidf'] = tfidf_feature(comment_text, cleaned_article_text)\n",
    "    feature_values['WordScore'] = wordscore_feature(comment_text, cleaned_article_text)\n",
    "    feature_values['WholeScore'] = wholescore_feature(comment_text, cleaned_article_text)\n",
    "    feature_values['adjWordScore'] = adjwordscore_feature(comment_text, cleaned_article_text)\n",
    "    \n",
    "    no_url_comment_text = remove_urls(comment_text)\n",
    "    \n",
    "    feature_values['no_url_WordScore'] = wordscore_feature(no_url_comment_text, cleaned_article_text)\n",
    "    feature_values['no_url_WholeScore'] = wholescore_feature(no_url_comment_text, cleaned_article_text)\n",
    "    \n",
    "    #Need to remove stop words here\n",
    "    \n",
    "    #Now call the rest of the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf',\n",
       " 'WordScore',\n",
       " 'WholeScore',\n",
       " 'contains_url',\n",
       " 'adjWordScore',\n",
       " 'no_url_WordScore',\n",
       " 'no_url_WholeScore',\n",
       " 'WordScoreNoStop',\n",
       " 'WholeScoreNoStop',\n",
       " 'no_url_or_stops_WholeScore',\n",
       " 'no_url_or_stops_WordScore']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple': False, 'banana': 0, 'orange': 4.9}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dit = {'apple': False, 'banana': 0, 'orange': 4.9}\n",
    "sample_dit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple': False, 'banana': True, 'orange': 4.9}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dit['banana'] = True\n",
    "sample_dit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
