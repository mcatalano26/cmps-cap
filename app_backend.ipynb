{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, this notebook can be implemented seamlessly into the web app's back end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To build the app backend, first import these libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mattc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "mattcat26\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "CLIENT_ID = os.getenv('CLIENT_ID')\n",
    "CLIENT_SECRET=os.getenv('CLIENT_SECRET')\n",
    "APP_NAME=os.getenv('APP_NAME')\n",
    "REDDIT_USERNAME=os.getenv('REDDIT_USERNAME')\n",
    "REDDIT_PASSWORD=os.getenv('REDDIT_PASSWORD')\n",
    "\n",
    "import praw\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator\n",
    "nlp.add_pipe(WordnetAnnotator(nlp.lang), after='tagger')\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "STOP_WORDS = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "from newspaper import Article\n",
    "from newspaper import Config\n",
    "\n",
    "user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'\n",
    "config = Config()\n",
    "config.browser_user_agent = user_agent\n",
    "\n",
    "reddit = praw.Reddit(client_id=CLIENT_ID, client_secret=CLIENT_SECRET, user_agent=APP_NAME, username=REDDIT_USERNAME, password=REDDIT_PASSWORD)\n",
    "\n",
    "print(reddit.user.me())\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next, include all of these functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_train_test_split(df, feature_list, target_name, test_size):\n",
    "    X = df[feature_list]\n",
    "    X = X.to_numpy()\n",
    "    y = df[target_name]\n",
    "    y = y.to_numpy()\n",
    "    rand_state = random.randint(0, 1000)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state=rand_state)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random Forest Classifier\n",
    "def random_forest_class_func(df, feature_list, target_name, test_size, estimators):\n",
    "    #set up training and testing split\n",
    "    X_train, X_test, y_train, y_test = set_up_train_test_split(df, feature_list, target_name, test_size)\n",
    "    \n",
    "    #fit ridge classifier to x and y training set\n",
    "    clf = RandomForestClassifier(n_estimators = estimators).fit(X_train, y_train)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_article(article_url):\n",
    "    try:\n",
    "        article = Article(article_url, language='en', fetch_images=False, config = config)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        art_text = article.text\n",
    "        art_doc = nlp(art_text.lower())\n",
    "    except:\n",
    "        print('The article could not be cleaned')\n",
    "    return art_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_url_feature(comment):\n",
    "    urlarr = []\n",
    "    urlarr = re.findall('[(]?http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', str(comment))\n",
    "    if not urlarr:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comment_text is the the comment and art_doc is the cleaned text of the article\n",
    "def wordscore_feature(comment_text, art_doc):\n",
    "    art_doc = nlp(str(art_doc))\n",
    "    art_items = [x.text for x in art_doc.ents]\n",
    "    #get tokens\n",
    "    art_tokens = []\n",
    "    for (item, count) in Counter(art_items).most_common(5):\n",
    "        token = nlp(item)[0]\n",
    "        art_tokens += [token]\n",
    "        \n",
    "    doc = nlp(str(comment_text).lower())\n",
    "    items = [x.text for x in doc.ents]\n",
    "    \n",
    "    score = 0\n",
    "    \n",
    "    for (item, count) in Counter(items).most_common(5):\n",
    "        \n",
    "        token = nlp(item)\n",
    "        \n",
    "        wordScores = []\n",
    "        \n",
    "        for art_word in art_tokens:\n",
    "            \n",
    "            wordScores += [art_word.similarity(token)]\n",
    "            \n",
    "            if len(wordScores) != 0:\n",
    "                score += sum(wordScores)/len(wordScores)\n",
    "            else:\n",
    "                score = 0\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comment_text is the the comment and art_doc is the cleaned text of the article\n",
    "def wholescore_feature(comment_text, art_doc):\n",
    "    art_doc = nlp(str(art_doc))\n",
    "    comment_text = str(comment_text).lower()\n",
    "    doc = nlp(comment_text)\n",
    "    score = art_doc.similarity(doc)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    urlarr = re.findall('[(]?http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', str(text))\n",
    "    for url in urlarr:\n",
    "        text = text.replace(url, '')\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = nlp(str(text))\n",
    "    token_list = []\n",
    "    for token in text:\n",
    "        token_list.append(token.text)\n",
    "        \n",
    "    filtered_text = ''\n",
    "    \n",
    "    for word in token_list:\n",
    "        lexeme = nlp.vocab[word]\n",
    "        if lexeme.is_stop == False:\n",
    "            if word in string.punctuation:\n",
    "                filtered_text += word\n",
    "            else:\n",
    "                filtered_text += (' ' + word)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjwordscore_feature(comment_text, art_doc):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_feature(comment_text, art_doc):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_feature(comment, article):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Send in comment text, reddit url, and feature list\n",
    "def big_func(comment_text, reddit_url, features, model):\n",
    "    feature_values = {'tfidf': 0, 'WordScore': 0, 'WholeScore': 0, 'contains_url': False, 'adjWordScore': 0, 'no_url_WordScore': 0, 'no_url_WholeScore': 0, 'WordScoreNoStop': 0, 'WholeScoreNoStop': 0, 'no_url_or_stops_WholeScore': 0, 'no_url_or_stops_WordScore': 0, 'NER_count': 0, 'NER_match': 0}\n",
    "    submission = reddit.submission(url = reddit_url)\n",
    "    article_url = submission.url\n",
    "    cleaned_article_text = clean_article(article_url)\n",
    "    \n",
    "    feature_values['contains_url'] = contains_url_feature(comment_text)\n",
    "    \n",
    "    #Need to figure out how to do tfidf\n",
    "    #feature_values['tfidf'] = tfidf_feature(comment_text, cleaned_article_text)\n",
    "    \n",
    "    feature_values['WordScore'] = wordscore_feature(comment_text, cleaned_article_text)\n",
    "    feature_values['WholeScore'] = wholescore_feature(comment_text, cleaned_article_text)\n",
    "    \n",
    "    #Need to include Gabe's adjusted word score feature here\n",
    "    #feature_values['adjWordScore'] = adjwordscore_feature(comment_text, cleaned_article_text)\n",
    "    \n",
    "    no_url_comment_text = remove_urls(comment_text)\n",
    "    no_url_article_text = remove_urls(cleaned_article_text)\n",
    "    \n",
    "    feature_values['no_url_WordScore'] = wordscore_feature(no_url_comment_text, no_url_article_text)\n",
    "    feature_values['no_url_WholeScore'] = wholescore_feature(no_url_comment_text, no_url_article_text)\n",
    "    \n",
    "    comment_text = remove_stopwords(comment_text)\n",
    "    cleaned_article_text = remove_stopwords(cleaned_article_text)\n",
    "    \n",
    "    feature_values['WordScoreNoStop'] = wordscore_feature(comment_text, cleaned_article_text)\n",
    "    feature_values['WholeScoreNoStop'] = wholescore_feature(comment_text, cleaned_article_text)\n",
    "\n",
    "    comment_text = remove_urls(comment_text)\n",
    "    cleaned_article_text = remove_urls(cleaned_article_text)\n",
    "    \n",
    "    feature_values['no_url_or_stops_WholeScore'] = wholescore_feature(comment_text, cleaned_article_text)\n",
    "    feature_values['no_url_or_stops_WordScore'] = wordscore_feature(comment_text, cleaned_article_text)\n",
    "    \n",
    "    #Need to include Sam's 2 new features, NER_count and NER_match\n",
    "    #feature_values['NER_count'], feature_values['NER_match'] = ner_feature(comment_text, cleaned_article_text)\n",
    "    \n",
    "    prelim_features = []\n",
    "    for feature in features:\n",
    "        prelim_features.append(feature_values[feature])\n",
    "    \n",
    "    prelim_features = [prelim_features]\n",
    "    prelim_features = np.array(prelim_features)\n",
    "    \n",
    "    prediction = model.predict(prelim_features)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lastly, run the next two boxes of code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = pd.read_csv('files/compiled_comments_3_14_2021.csv')\n",
    "features = ['WordScore', 'WholeScore', 'contains_url', 'no_url_WordScore', 'no_url_WholeScore', 'WordScoreNoStop', 'WholeScoreNoStop', 'no_url_or_stops_WholeScore', 'no_url_or_stops_WordScore']\n",
    "# features = ['WordScore', 'WholeScore', 'contains_url', 'no_url_WordScore', 'no_url_WholeScore', 'WordScoreNoStop', 'WholeScoreNoStop', 'no_url_or_stops_WholeScore', 'no_url_or_stops_WordScore', 'NER_count', 'NER_match', 'tfidf', 'adjWordScore']\n",
    "our_model = random_forest_class_func(comments_df, features, 'action', 0.1, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, get the features for the new comment/article and make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad comment\n"
     ]
    }
   ],
   "source": [
    "#To demonstrate the backend, we will hardcode a comment and article\n",
    "bad_comment = 'bad'\n",
    "good_comment = 'In a statement to NBC News, the Office of tje Director of National Intelligence said it will not interface with the Biden transition until the General Services Administration decides its clear who won'\n",
    "reddit_url = 'https://www.reddit.com/r/neutralnews/comments/jrts8z/biden_not_getting_intel_reports_because_trump/'\n",
    "\n",
    "comment = bad_comment\n",
    "answer = big_func(comment, reddit_url, features, our_model)[0]\n",
    "\n",
    "if answer:\n",
    "    print('Good comment')\n",
    "else:\n",
    "    print('Bad comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple': False, 'banana': 0, 'orange': 4.9}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dit = {'apple': False, 'banana': 0, 'orange': 4.9}\n",
    "sample_dit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple': False, 'banana': True, 'orange': 4.9}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dit['banana'] = True\n",
    "sample_dit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.zeros(1)\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8,  9, 10])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = [8, 9, 10]\n",
    "arr = np.array(arr)\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [8, 9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 9, 10]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
