{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "CLIENT_ID = os.getenv('CLIENT_ID')\n",
    "CLIENT_SECRET=os.getenv('CLIENT_SECRET')\n",
    "APP_NAME=os.getenv('APP_NAME')\n",
    "REDDIT_USERNAME=os.getenv('REDDIT_USERNAME')\n",
    "REDDIT_PASSWORD=os.getenv('REDDIT_PASSWORD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Reddit API stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id=CLIENT_ID, client_secret=CLIENT_SECRET, user_agent=APP_NAME, username=REDDIT_USERNAME, password=REDDIT_PASSWORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get dataframe of topics from neutral news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ListingGenerator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-97f32da827d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mtopics_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'body'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubmission\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselftext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mtopics_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopics_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mtop_subreddit\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'ListingGenerator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "subreddit = reddit.subreddit('neutralnews')\n",
    "top_subreddit = subreddit.top()\n",
    "topics_dict = {'title':[], 'score':[], 'id':[], 'url':[], 'comms_num': [], 'created':[], 'body':[]}\n",
    "for submission in top_subreddit:\n",
    "    topics_dict['title'].append(submission.title)\n",
    "    topics_dict['score'].append(submission.score)\n",
    "    topics_dict['id'].append(submission.id)\n",
    "    topics_dict['url'].append(submission.url)\n",
    "    topics_dict['comms_num'].append(submission.num_comments)\n",
    "    topics_dict['created'].append(submission.created)\n",
    "    topics_dict['body'].append(submission.selftext)\n",
    "topics_data = pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make comment df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_dict = {\"action\": [], \"content\": [], \"author\": [], \"details\": [], \"submissionId\": [], \"commentId\": []}\n",
    "for submission in subreddit.top(limit=20):\n",
    "    #print(submission.title, submission.id)\n",
    "    submission.comments.replace_more(limit=100)\n",
    "    for comment in submission.comments:\n",
    "        #print(top_level_comment.body)\n",
    "    \n",
    "        comments_dict[\"action\"].append(np.nan)\n",
    "        comments_dict[\"content\"].append(comment.body)\n",
    "        comments_dict[\"author\"].append(comment.author)\n",
    "        comments_dict[\"details\"].append(np.nan)\n",
    "        comments_dict[\"submissionId\"].append(submission.id)\n",
    "        comments_dict[\"commentId\"].append(comment.id)\n",
    "\n",
    "comment_data = pd.DataFrame(comments_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action</th>\n",
       "      <th>content</th>\n",
       "      <th>author</th>\n",
       "      <th>details</th>\n",
       "      <th>submissionId</th>\n",
       "      <th>commentId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>---- **/r/NeutralNews is a curated space.**\\nI...</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5uy6s0</td>\n",
       "      <td>ddxqtmg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just a quick reminder what neutral means here ...</td>\n",
       "      <td>BundleOfHiss</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5uy6s0</td>\n",
       "      <td>ddy27rm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Already on the campaign trail Trump wanted to ...</td>\n",
       "      <td>samuelsamvimes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5uy6s0</td>\n",
       "      <td>ddxttda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This may be true but criticizing the press isn...</td>\n",
       "      <td>RufusRocks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5uy6s0</td>\n",
       "      <td>ddxzpjb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ever since [propaganda](http://www.businessins...</td>\n",
       "      <td>cheekygorilla</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5uy6s0</td>\n",
       "      <td>ddyajye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What bill are they actually talking about? I’d...</td>\n",
       "      <td>HarryPotterAMA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7ee4u9</td>\n",
       "      <td>dq5hn1i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7ee4u9</td>\n",
       "      <td>dq4ws7x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Doing their darndest to hold onto that Alabama...</td>\n",
       "      <td>Ginger_Lord</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7ee4u9</td>\n",
       "      <td>dq5gczf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>NaN</td>\n",
       "      <td>---- **/r/NeutralNews is a curated space.**\\nI...</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ahxiba</td>\n",
       "      <td>eej17bt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This is basically the poll [that the president...</td>\n",
       "      <td>TDaltonC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ahxiba</td>\n",
       "      <td>eejf1ke</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>238 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     action                                            content  \\\n",
       "0       NaN  ---- **/r/NeutralNews is a curated space.**\\nI...   \n",
       "1       NaN  Just a quick reminder what neutral means here ...   \n",
       "2       NaN  Already on the campaign trail Trump wanted to ...   \n",
       "3       NaN  This may be true but criticizing the press isn...   \n",
       "4       NaN  Ever since [propaganda](http://www.businessins...   \n",
       "..      ...                                                ...   \n",
       "233     NaN  What bill are they actually talking about? I’d...   \n",
       "234     NaN                                          [removed]   \n",
       "235     NaN  Doing their darndest to hold onto that Alabama...   \n",
       "236     NaN  ---- **/r/NeutralNews is a curated space.**\\nI...   \n",
       "237     NaN  This is basically the poll [that the president...   \n",
       "\n",
       "             author  details submissionId commentId  \n",
       "0     AutoModerator      NaN       5uy6s0   ddxqtmg  \n",
       "1      BundleOfHiss      NaN       5uy6s0   ddy27rm  \n",
       "2    samuelsamvimes      NaN       5uy6s0   ddxttda  \n",
       "3        RufusRocks      NaN       5uy6s0   ddxzpjb  \n",
       "4     cheekygorilla      NaN       5uy6s0   ddyajye  \n",
       "..              ...      ...          ...       ...  \n",
       "233  HarryPotterAMA      NaN       7ee4u9   dq5hn1i  \n",
       "234            None      NaN       7ee4u9   dq4ws7x  \n",
       "235     Ginger_Lord      NaN       7ee4u9   dq5gczf  \n",
       "236   AutoModerator      NaN       ahxiba   eej17bt  \n",
       "237        TDaltonC      NaN       ahxiba   eejf1ke  \n",
       "\n",
       "[238 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import NER and Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_lg==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.3.1/en_core_web_lg-2.3.1.tar.gz#egg=en_core_web_lg==2.3.1 in c:\\users\\mattc\\anaconda3\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in c:\\users\\mattc\\anaconda3\\lib\\site-packages (from en_core_web_lg==2.3.1) (2.3.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\mattc\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\mattc\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\mattc\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (4.50.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\mattc\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\mattc\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (2.0.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\mattc\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mattc\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (41.4.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\mattc\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (1.16.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\mattc\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (2.22.0)\n",
      "Requirement already satisfied: thinc==7.4.1 in c:\\users\\mattc\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\users\\mattc\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\mattc\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\mattc\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (3.0.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\mattc\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mattc\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\mattc\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\mattc\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (2.8)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\users\\mattc\\anaconda3\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (0.23)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\mattc\\anaconda3\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\mattc\\anaconda3\\lib\\site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (7.2.0)\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_lg')\n",
      "Requirement already satisfied: spacy-wordnet in c:\\users\\mattc\\anaconda3\\lib\\site-packages (0.0.4)\n",
      "Requirement already satisfied: nltk<3.4,>=3.3 in c:\\users\\mattc\\anaconda3\\lib\\site-packages (from spacy-wordnet) (3.3)\n",
      "Requirement already satisfied: six in c:\\users\\mattc\\anaconda3\\lib\\site-packages (from nltk<3.4,>=3.3->spacy-wordnet) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "from newspaper import Article\n",
    "from newspaper import Config\n",
    "\n",
    "user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'\n",
    "config = Config()\n",
    "config.browser_user_agent = user_agent\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "!python -m spacy download en_core_web_lg\n",
    "!pip install spacy-wordnet\n",
    "\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator\n",
    "nlp.add_pipe(WordnetAnnotator(nlp.lang), after='tagger')\n",
    "\n",
    "\n",
    "STOP_WORDS = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_articles(topics_data, comment_data, text_list):\n",
    "    for url in topics_data['url']:\n",
    "        try:\n",
    "            article = Article(submission.url, language='en', fetch_images=False, config = config)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            art_text = article.text\n",
    "            art_doc = nlp(art_text.lower())\n",
    "            text_list.append(art_doc)\n",
    "        #if there is an exception, remove comments that go with this article\n",
    "        except:\n",
    "            text_list.append(\"error\")\n",
    "            comment_data = comment_data[comment_data['submissionId'] != url]\n",
    "            continue\n",
    "    \n",
    "    topics_data['text'] = text_list\n",
    "    \n",
    "clean_articles(topics_data, comment_data, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'ents'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-2770380112f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msimWordScore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mwordScoreList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetSimWordScore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomment_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopics_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[0mcomment_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'WordScore'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordScoreList\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-2770380112f3>\u001b[0m in \u001b[0;36mgetSimWordScore\u001b[1;34m(comment_data, topics_data, simWordScore)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mpost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtopics_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtopics_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0msubID\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mart_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpost\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mart_items\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mart_doc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ments\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;31m#get tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mart_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5177\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5178\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5179\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5181\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'ents'"
     ]
    }
   ],
   "source": [
    "def getSimWordScore(comment_data, topics_data, simWordScore):\n",
    "    for index, comment in comment_data.iterrows():\n",
    "        subID = comment['submissionId'] #get submission id from comment table\n",
    "        post = topics_data[topics_data['id'] == subID]\n",
    "        art_doc = post['text']\n",
    "        art_items = [x.text for x in art_doc.ents]\n",
    "        #get tokens\n",
    "        art_tokens = []\n",
    "        for (item, count) in Counter(art_items).most_common(5):\n",
    "            token = nlp(item)[0]\n",
    "            art_tokens += [token]\n",
    "        #get comment content and ner\n",
    "        comment_text = comment['content']\n",
    "        doc = nlp(comment_text.lower())\n",
    "\n",
    "        items = [x.text for x in doc.ents]\n",
    "\n",
    "        #initialize list of scores\n",
    "        score = 0\n",
    "        #for each token, get a score\n",
    "        for (item, count) in Counter(items).most_common(5):\n",
    "\n",
    "            #get token\n",
    "            token = nlp(item)#[0]\n",
    "\n",
    "            wordScores = []\n",
    "\n",
    "            #for each article item\n",
    "            for art_word in art_tokens:\n",
    "\n",
    "                #add similarity score to list of scores\n",
    "                wordScores += [art_word.similarity(token)]\n",
    "            #get average score    \n",
    "            score += sum(wordScores)/len(wordScores)\n",
    "        simWordScore.append(score)\n",
    "    return simWordScore\n",
    "\n",
    "wordScoreList = getSimWordScore(comment_data, topics_data, [])\n",
    "comment_data['WordScore'] = wordScoreList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run NER to get similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currID = ''\n",
    "simWordScore = []\n",
    "simWholeScore = []\n",
    "for i in range(len(comment_data)):\n",
    "    subID = comment_data.iloc[i]['submissionId'] #get submission id from comment table\n",
    "    \n",
    "    #if this submission id is different than the current submission id\n",
    "    if subID != currID:\n",
    "        #get the submission and set current ID to this ID\n",
    "        submission = reddit.submission(subID)\n",
    "        currID = subID\n",
    "        \n",
    "        #get article from url, and begin NER\n",
    "        #try to get article\n",
    "        try:\n",
    "            article = Article(submission.url, language='en', fetch_images=False, config = config)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            art_text = article.text\n",
    "            art_doc = nlp(art_text.lower())\n",
    "        \n",
    "        #if there is an exception, remove comments that go with this article\n",
    "        except:\n",
    "            i = i + len(comment_data['submissionId'] == currID) - 1\n",
    "            comment_data = comment_data[comment_data['submissionId'] != currID]\n",
    "            continue\n",
    "            \n",
    "        art_items = [x.text for x in art_doc.ents]\n",
    "        #get tokens\n",
    "        art_tokens = []\n",
    "        for (item, count) in Counter(art_items).most_common(5):\n",
    "            token = nlp(item)[0]\n",
    "            art_tokens += [token]\n",
    "    \n",
    "    #get comment content and ner\n",
    "    comment = comment_data.iloc[i]['content']\n",
    "    doc = nlp(comment.lower())\n",
    "    \n",
    "    simWholeScore.append(art_doc.similarity(doc))\n",
    "    \n",
    "    items = [x.text for x in doc.ents]\n",
    "    \n",
    "    #initialize list of scores\n",
    "    score = 0\n",
    "    #for each token, get a score\n",
    "    for (item, count) in Counter(items).most_common(5):\n",
    "        \n",
    "        #get token\n",
    "        token = nlp(item)#[0]\n",
    "        \n",
    "        wordScores = []\n",
    "        \n",
    "        #for each article item\n",
    "        for art_word in art_tokens:\n",
    "            \n",
    "            #add similarity score to list of scores\n",
    "            wordScores += [art_word.similarity(token)]\n",
    "        #get average score    \n",
    "        score += sum(wordScores)/len(wordScores)\n",
    "    simWordScore.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimWholeScore(currID, simWholeScore):\n",
    "    subID = comment_data.iloc[i]['submissionId'] #get submission id from comment table\n",
    "\n",
    "    #if this submission id is different than the current submission id\n",
    "    if subID != currID:\n",
    "        #get the submission and set current ID to this ID\n",
    "        submission = reddit.submission(subID)\n",
    "        currID = subID\n",
    "\n",
    "        #get article from url, and begin NER\n",
    "        #try to get article\n",
    "        try:\n",
    "            article = Article(submission.url, language='en', fetch_images=False, config = config)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            art_text = article.text\n",
    "            art_doc = nlp(art_text.lower())\n",
    "\n",
    "        #if there is an exception, remove comments that go with this article\n",
    "        except:\n",
    "            i = i + len(comment_data['submissionId'] == currID) - 1\n",
    "            comment_data = comment_data[comment_data['submissionId'] != currID]\n",
    "            continue\n",
    "            \n",
    "    #get comment content and ner\n",
    "    comment = comment_data.iloc[i]['content']\n",
    "    doc = nlp(comment.lower())\n",
    "\n",
    "    simWholeScore.append(art_doc.similarity(doc))\n",
    "wholeScoreList = getSimWholeScore(comment_data, topics_data, [])\n",
    "comment_data['WholeScore'] = wholeScoreList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = ['4o2o29',\n",
    " '4op948',\n",
    " '4sef35',\n",
    " '5x0k84',\n",
    " '64zsim',\n",
    " '68qqfz',\n",
    " '6907e3',\n",
    " '6mmw91',\n",
    " '6n161i',\n",
    " '6oftmx',\n",
    " '6xapy0',\n",
    " '7307wv',\n",
    " '76z4xh',\n",
    " '7ht8tj',\n",
    " '7hyg56',\n",
    " '7z5dyt',\n",
    " '804o3f',\n",
    " '862sn6',\n",
    " '8d0t2m',\n",
    " '8e7lrz',\n",
    " '8go559',\n",
    " '8hz10r',\n",
    " '8uby76',\n",
    " '8x2571',\n",
    " '8zp0uh',\n",
    " '91gwrp',\n",
    " '94q8j7',\n",
    " '96nrmf',\n",
    " '99fccg',\n",
    " '9c88xn',\n",
    " '9f2i6b',\n",
    " '9fj3m8',\n",
    " '9h0mo6',\n",
    " '9h82yb',\n",
    " '9ht83g',\n",
    " '9iqed1',\n",
    " '9j3gvd',\n",
    " '9jpdh7',\n",
    " '9kq8xy',\n",
    " '9ood1n',\n",
    " '9rbpt8',\n",
    " '9s3gd8',\n",
    " '9smhsl',\n",
    " '9xyyt0',\n",
    " '9zonfn',\n",
    " 'a4tn3i',\n",
    " 'a67z1j',\n",
    " 'aab5gq',\n",
    " 'ab7m8y',\n",
    " 'ac7d56',\n",
    " 'acz2f9',\n",
    " 'ahxiba',\n",
    " 'ailn92',\n",
    " 'aj96gr',\n",
    " 'ajvsq6',\n",
    " 'alzqd0',\n",
    " 'an9m5u',\n",
    " 'andfa8',\n",
    " 'apss2q',\n",
    " 'asacv6',\n",
    " 'asrdao',\n",
    " 'auz29y',\n",
    " 'axnhrj',\n",
    " 'b47a1d',\n",
    " 'b7e8o4',\n",
    " 'b8cd1q',\n",
    " 'b9d62h',\n",
    " 'bj3tgb',\n",
    " 'bjngf5',\n",
    " 'bka66t',\n",
    " 'blxd09',\n",
    " 'bpi7u6',\n",
    " 'busrz0',\n",
    " 'bv8dl8',\n",
    " 'bvk72a',\n",
    " 'bxtj3y',\n",
    " 'byiwll',\n",
    " 'c109ai',\n",
    " 'c3swuk',\n",
    " 'c75b2i',\n",
    " 'c7nqp7',\n",
    " 'cess35',\n",
    " 'cfa7tc',\n",
    " 'cfa99a',\n",
    " 'cfc5qa',\n",
    " 'cfzkky',\n",
    " 'cgd7ut',\n",
    " 'cgks97',\n",
    " 'cgkspq',\n",
    " 'cgktg9',\n",
    " 'cgkueu',\n",
    " 'cgl6x6',\n",
    " 'cim6kf',\n",
    " 'f8says',\n",
    " 'fd2f6z',\n",
    " 'fuhknm',\n",
    " 'gcz931',\n",
    " 'gw3uey',\n",
    " 'h9pyv5',\n",
    " 'hi25c6',\n",
    " 'hi274b',\n",
    " 'hi2ab8',\n",
    " 'hi2bdj',\n",
    " 'hi2ckj',\n",
    " 'hi3rm3',\n",
    " 'hi3vwz',\n",
    " 'hi3wk8',\n",
    " 'hi4bgg',\n",
    " 'hi4evw',\n",
    " 'hi4x3g',\n",
    " 'hibdl7',\n",
    " 'hibxec',\n",
    " 'hic2by',\n",
    " 'hic40i',\n",
    " 'hiepzp',\n",
    " 'hioh3o',\n",
    " 'hiokda',\n",
    " 'hirg3c',\n",
    " 'hivrgb',\n",
    " 'hiwou8',\n",
    " 'hj1nrq',\n",
    " 'hjbexy',\n",
    " 'hjbkf4',\n",
    " 'hjimwh',\n",
    " 'hjjxr5',\n",
    " 'hjlda4',\n",
    " 'hjm3wf',\n",
    " 'hjm6py',\n",
    " 'hjukp3',\n",
    " 'hjxeu0',\n",
    " 'hk0qme',\n",
    " 'hk2e32',\n",
    " 'hk6kwj',\n",
    " 'hkjxz4',\n",
    " 'hkoist',\n",
    " 'hkt524',\n",
    " 'hkx96j',\n",
    " 'hl6bt7',\n",
    " 'hl9cux',\n",
    " 'hlaf18',\n",
    " 'hlnh4l',\n",
    " 'hlp04m',\n",
    " 'hlpg7q',\n",
    " 'hlxf66',\n",
    " 'hly9t3',\n",
    " 'hm7yu1',\n",
    " 'hm8nfw',\n",
    " 'hm8qvv',\n",
    " 'hm9zu6',\n",
    " 'hmhvvu',\n",
    " 'hmk5k2',\n",
    " 'hmor9m',\n",
    " 'hmqkok',\n",
    " 'hmy4jo',\n",
    " 'hn1me3',\n",
    " 'hnh39s',\n",
    " 'hnhbls',\n",
    " 'hnid7x',\n",
    " 'hnovi8',\n",
    " 'hnqx5w',\n",
    " 'ho28c4',\n",
    " 'ho3359',\n",
    " 'ho42my',\n",
    " 'ho59s9',\n",
    " 'ho5jt8',\n",
    " 'ho63fl',\n",
    " 'hoo2zh',\n",
    " 'horcro',\n",
    " 'hosttw',\n",
    " 'hp0ehw',\n",
    " 'hp0eso',\n",
    " 'hpjk4b',\n",
    " 'hpx1r9',\n",
    " 'hqbqsp',\n",
    " 'hqfa6c',\n",
    " 'hqj618',\n",
    " 'hr3a77',\n",
    " 'hr8126',\n",
    " 'hr9uva',\n",
    " 'hrcipo',\n",
    " 'hrf0fa',\n",
    " 'hrmbwc',\n",
    " 'hrmo0b',\n",
    " 'hrpxqn',\n",
    " 'hruu5h',\n",
    " 'hs5x4y',\n",
    " 'hsacrg',\n",
    " 'hsal1n',\n",
    " 'hse1gz',\n",
    " 'hsjsqx',\n",
    " 'hstslg',\n",
    " 'hsvpjz',\n",
    " 'hsw9jm',\n",
    " 'ht1vcp',\n",
    " 'ht7wcf',\n",
    " 'htasdr',\n",
    " 'hti9ls',\n",
    " 'htol34',\n",
    " 'htrt98',\n",
    " 'hu0n3u',\n",
    " 'hu2rko',\n",
    " 'huke8e',\n",
    " 'hul38u',\n",
    " 'hul9at',\n",
    " 'hulpli',\n",
    " 'huqzl7',\n",
    " 'huw0tt',\n",
    " 'huw3oh',\n",
    " 'hv149r',\n",
    " 'hv6rn4',\n",
    " 'hvczsj',\n",
    " 'hvdh5m',\n",
    " 'hvgskd',\n",
    " 'hvhcop',\n",
    " 'hvtzb6',\n",
    " 'hw2lr9',\n",
    " 'hwakq3',\n",
    " 'hwawu3',\n",
    " 'hwg5me',\n",
    " 'hwjoxu',\n",
    " 'hx0e81',\n",
    " 'hx9vlf',\n",
    " 'hxfb5w',\n",
    " 'hxpm0c',\n",
    " 'hxq0u6',\n",
    " 'hxypm9',\n",
    " 'hy64fx',\n",
    " 'hyf828',\n",
    " 'hyg7f6',\n",
    " 'hyscrn',\n",
    " 'hyta9l',\n",
    " 'hyubiw',\n",
    " 'hyxdk8',\n",
    " 'hzevob',\n",
    " 'hzhcai',\n",
    " 'hzl8el',\n",
    " 'hzp4ts',\n",
    " 'hztuwl',\n",
    " 'i00tfi',\n",
    " 'i02pzj',\n",
    " 'i03xkr',\n",
    " 'i07pbd',\n",
    " 'i0m39k',\n",
    " 'i0mxrh',\n",
    " 'i0n6q4',\n",
    " 'i0nct3',\n",
    " 'i12kut',\n",
    " 'i16cum',\n",
    " 'i17yor',\n",
    " 'i18yzj',\n",
    " 'i1gv43',\n",
    " 'i1hkfg',\n",
    " 'i1syu7',\n",
    " 'i21mky',\n",
    " 'i21pw5',\n",
    " 'i22uxu',\n",
    " 'i28qla',\n",
    " 'i28yme',\n",
    " 'i292mp',\n",
    " 'i2bp46',\n",
    " 'i2dogn',\n",
    " 'i2f8b3',\n",
    " 'i33x8h',\n",
    " 'i38x2w',\n",
    " 'i3il30',\n",
    " 'i3jcbd',\n",
    " 'i3jfjv',\n",
    " 'i3k5uu',\n",
    " 'i3k6x6',\n",
    " 'i3uttm',\n",
    " 'i3wcwt',\n",
    " 'i46ep0',\n",
    " 'i4fnx0',\n",
    " 'i4jlno',\n",
    " 'i4l57l',\n",
    " 'i4lo6w',\n",
    " 'i4qm6n',\n",
    " 'i4ucjp',\n",
    " 'i5d7ed',\n",
    " 'i5e8p7',\n",
    " 'i5ex5l',\n",
    " 'i5p8xz',\n",
    " 'i5r8av',\n",
    " 'i6ijq7',\n",
    " 'i6oah7',\n",
    " 'i6q3rl',\n",
    " 'i6s6e4',\n",
    " 'i6uvoc',\n",
    " 'i739ub',\n",
    " 'i74yaf',\n",
    " 'i75f5j',\n",
    " 'i77lwd',\n",
    " 'i7a2fj',\n",
    " 'i7gahj',\n",
    " 'i7q4sp',\n",
    " 'i7safq',\n",
    " 'i7such',\n",
    " 'i7wc60',\n",
    " 'i7zc4g',\n",
    " 'i7zica',\n",
    " 'i80kio',\n",
    " 'i8drdk',\n",
    " 'i8dxuw',\n",
    " 'i8nfh1',\n",
    " 'i8oo52',\n",
    " 'i8w5x2',\n",
    " 'i9210g',\n",
    " 'i929nd',\n",
    " 'i9d54j',\n",
    " 'i9mtu0',\n",
    " 'i9otlg',\n",
    " 'i9smf6',\n",
    " 'i9vled',\n",
    " 'ia0qvf',\n",
    " 'ia7d83',\n",
    " 'ia8tf4',\n",
    " 'iabirm',\n",
    " 'iakuxb',\n",
    " 'iaua5h',\n",
    " 'ib12pg',\n",
    " 'ib2axf',\n",
    " 'ib4jlg',\n",
    " 'ibd7pw',\n",
    " 'ibfjjq',\n",
    " 'ibg1d0',\n",
    " 'ibnlsa',\n",
    " 'ibyrmx',\n",
    " 'ic10hd',\n",
    " 'ic1wap',\n",
    " 'ic2sln',\n",
    " 'ic4w7z',\n",
    " 'ice94z',\n",
    " 'ichwt4',\n",
    " 'icmpee',\n",
    " 'icnnua',\n",
    " 'ics0fc',\n",
    " 'icxrsk',\n",
    " 'icy3z3',\n",
    " 'id3ug9',\n",
    " 'id6naf',\n",
    " 'idcpll',\n",
    " 'ie0flp',\n",
    " 'ie53mq',\n",
    " 'ieb9em',\n",
    " 'ieje3j',\n",
    " 'ieoobz',\n",
    " 'iepdjp',\n",
    " 'iewpfo',\n",
    " 'if5nbq',\n",
    " 'ifij8p',\n",
    " 'ifp07u',\n",
    " 'ifq1l9',\n",
    " 'ifty7c',\n",
    " 'ifwln1',\n",
    " 'igc7uc',\n",
    " 'igo5yh',\n",
    " 'igpjip',\n",
    " 'igqpm2',\n",
    " 'igwngr',\n",
    " 'ih1d3o',\n",
    " 'ih2rah',\n",
    " 'ih481a',\n",
    " 'ih9mqu',\n",
    " 'ihf2wt',\n",
    " 'ihiuif',\n",
    " 'ihnvuu',\n",
    " 'ihpxh7',\n",
    " 'ihsoih',\n",
    " 'ihz2hg',\n",
    " 'ii9aju',\n",
    " 'ii9z1s',\n",
    " 'iicu9x',\n",
    " 'iifznc',\n",
    " 'iinnom',\n",
    " 'iiyfmg',\n",
    " 'ij02pj',\n",
    " 'ijgy6t',\n",
    " 'ijn55e',\n",
    " 'ijnwn0',\n",
    " 'ijxzxa',\n",
    " 'ik07zh',\n",
    " 'ik1agl',\n",
    " 'ik4afi',\n",
    " 'ikbrh9',\n",
    " 'ikh1vf',\n",
    " 'ikjcx2',\n",
    " 'ikolfk',\n",
    " 'iktlis',\n",
    " 'ikv1gj',\n",
    " 'il0iua',\n",
    " 'il5vfn',\n",
    " 'il6dom',\n",
    " 'il6q46',\n",
    " 'il6sty',\n",
    " 'il742b',\n",
    " 'il7in0',\n",
    " 'il9n4p',\n",
    " 'ilajy8',\n",
    " 'ilba8s',\n",
    " 'ilbgd7',\n",
    " 'ilbv07',\n",
    " 'ilenv6',\n",
    " 'ilg27f',\n",
    " 'ilgw6c',\n",
    " 'ilh8t5',\n",
    " 'ilhx68',\n",
    " 'ilicps',\n",
    " 'ilixe5',\n",
    " 'iljeq8',\n",
    " 'ilk2hb',\n",
    " 'illuuz',\n",
    " 'ilozqt',\n",
    " 'ilwrx6',\n",
    " 'ily09p',\n",
    " 'im36cp',\n",
    " 'im62jx',\n",
    " 'im6tnl',\n",
    " 'im8jmm',\n",
    " 'imbj93',\n",
    " 'imkbta',\n",
    " 'imkt2m',\n",
    " 'immelc',\n",
    " 'imo2d5',\n",
    " 'imorci',\n",
    " 'imsl4y',\n",
    " 'imufs1',\n",
    " 'in14m8',\n",
    " 'ink4j7',\n",
    " 'inm0is',\n",
    " 'inqhx7',\n",
    " 'io7gfn',\n",
    " 'ioluws',\n",
    " 'iovspa',\n",
    " 'iowrrf',\n",
    " 'ioxz26',\n",
    " 'ip7po2',\n",
    " 'ipci0w',\n",
    " 'ipgnvk',\n",
    " 'iphfy0',\n",
    " 'ipjzsk',\n",
    " 'ipmgnu',\n",
    " 'iq2ppp',\n",
    " 'iq3aul',\n",
    " 'iq4crh',\n",
    " 'iqbfy1',\n",
    " 'iqus5t',\n",
    " 'iqw6zx',\n",
    " 'ir69ug',\n",
    " 'iratrz',\n",
    " 'irm7ex',\n",
    " 'irx2mk',\n",
    " 'irxd3c',\n",
    " 'irzhex',\n",
    " 'is1rno',\n",
    " 'is5xo0',\n",
    " 'isaozz',\n",
    " 'isbhkv',\n",
    " 'isj07n',\n",
    " 'isjxc3',\n",
    " 'ismoay',\n",
    " 'ispld0',\n",
    " 'isqv6m',\n",
    " 'istm8e',\n",
    " 'isum8d',\n",
    " 'isupz8',\n",
    " 'it7r7w',\n",
    " 'it8wz9',\n",
    " 'it9q8d',\n",
    " 'itdjjf',\n",
    " 'itgby4',\n",
    " 'itmffo',\n",
    " 'ituljf',\n",
    " 'itys1b',\n",
    " 'iu0b63',\n",
    " 'iu4oda',\n",
    " 'iu914j',\n",
    " 'iujhay',\n",
    " 'iujn20',\n",
    " 'iupmy2',\n",
    " 'iuqbld',\n",
    " 'iuu472',\n",
    " 'iuvl82',\n",
    " 'iv3az1',\n",
    " 'iv534g',\n",
    " 'iv5s63',\n",
    " 'iv6drn',\n",
    " 'iv6t7c',\n",
    " 'ivgtgl',\n",
    " 'ivhf6s',\n",
    " 'ivjfcw',\n",
    " 'ivn7hd',\n",
    " 'ivskes',\n",
    " 'ivy8qp',\n",
    " 'ivys3w',\n",
    " 'iwdven',\n",
    " 'iwh281',\n",
    " 'iwmawa',\n",
    " 'iwohni',\n",
    " 'iwse12',\n",
    " 'ix2wbk',\n",
    " 'ix2xbx',\n",
    " 'ix3rw3',\n",
    " 'ix409e',\n",
    " 'ix6rxb',\n",
    " 'ixllnj',\n",
    " 'ixmqbx',\n",
    " 'ixqs7j',\n",
    " 'ixtt0y',\n",
    " 'ixwxg6',\n",
    " 'iy2mv7',\n",
    " 'iy2phk',\n",
    " 'iy9574',\n",
    " 'iydbkj',\n",
    " 'iyfyeu',\n",
    " 'iyjm83',\n",
    " 'iyoo0x',\n",
    " 'iyorls',\n",
    " 'iz7am2',\n",
    " 'iz951v',\n",
    " 'izf05k',\n",
    " 'izijv0',\n",
    " 'izktke',\n",
    " 'izku6m',\n",
    " 'izmzt3',\n",
    " 'izp6yc',\n",
    " 'izrbbb',\n",
    " 'izt4sk',\n",
    " 'izul0p',\n",
    " 'j04eqv',\n",
    " 'j06hcq',\n",
    " 'j06lwq',\n",
    " 'j078ii',\n",
    " 'j099m5',\n",
    " 'j09b0v',\n",
    " 'j0h85d',\n",
    " 'j0ijwn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "536"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['banana', 'apple']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_arr = ['banana']\n",
    "new_arr += ['apple']\n",
    "new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
